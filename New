import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from transformers import pipeline
from bertviz import head_view
import pandas as pd
from matplotlib.colors import LinearSegmentedColormap

# Set up the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the model and tokenizer
model_name = "xlm-roberta-base"
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)
model.to(device)

# Create a sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

def get_attention_scores(text, model, tokenizer):
    """Get attention scores for all layers and heads from the model."""
    # Tokenize input
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Get model outputs with attention matrices
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
    
    # Get attention outputs and convert to numpy arrays
    attentions = outputs.attentions
    attention_matrices = [attention.cpu().numpy() for attention in attentions]
##############################
def consolidate_vectors(key_to_words, word_to_vector, target_key, aggregation_method='sum'):
    """
    Consolidate vectors for all words associated with a target key.
    
    Parameters:
    - key_to_words: Dictionary mapping keys to lists of words
    - word_to_vector: Dictionary mapping words to vector tuples
    - target_key: The key whose associated word vectors will be consolidated
    - aggregation_method: How to combine vectors ('sum', 'average', 'max', 'min')
    
    Returns:
    - Consolidated vector as a tuple
    """
    # Get all words associated with the target key
    if target_key not in key_to_words:
        return None
    
    words = key_to_words[target_key]
    
    # Collect vectors for all valid words
    vectors = []
    for word in words:
        if word in word_to_vector:
            vectors.append(word_to_vector[word])
    
    if not vectors:
        return None
    
    # Convert tuples to lists for easier manipulation
    vector_lists = [list(v) for v in vectors]
    
    # Get the length of vectors (assuming all have same length)
    vector_length = len(vector_lists[0])
    
    # Perform the aggregation based on the specified method
    if aggregation_method == 'sum':
        result = [sum(v[i] for v in vector_lists) for i in range(vector_length)]
    elif aggregation_method == 'average':
        result = [sum(v[i] for v in vector_lists) / len(vectors) for i in range(vector_length)]
    elif aggregation_method == 'max':
        result = [max(v[i] for v in vector_lists) for i in range(vector_length)]
    elif aggregation_method == 'min':
        result = [min(v[i] for v in vector_lists) for i in range(vector_length)]
    else:
        raise ValueError(f"Unknown aggregation method: {aggregation_method}")
    
    return tuple(result)
Here's an example of how to use this function:
python# Example dictionaries
key_to_words = {
    'animals': ['cat', 'dog', 'bird'],
    'colors': ['red', 'blue', 'green'],
    'numbers': ['one', 'two', 'three']
}

word_to_vector = {
    'cat': (1, 2, 3),
    'dog': (4, 5, 6),
    'bird': (7, 8, 9),
    'red': (10, 20, 30),
    'blue': (40, 50, 60),
    'one': (100, 200, 300)
}

# Consolidate vectors for 'animals' key using default 'sum' method
result = consolidate_vectors(key_to_words, word_to_vector, 'animals')
print(f"Sum of vectors for 'animals': {result}")
    
    # Get token ids and convert to tokens
    input_ids = inputs["input_ids"][0].cpu().numpy()
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    return attention_matrices, tokens, outputs.logits

def plot_attention_heatmap(attention_matrices, tokens, layer_idx, head_idx, cmap='viridis'):
    """Plot attention heatmap for a specific layer and attention head."""
    # Get attention matrix for specific layer and head
    attention = attention_matrices[layer_idx][0, head_idx]
    
    # Create a figure
    plt.figure(figsize=(12, 10))
    
    # Plot heatmap
    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap=cmap)
    
    plt.title(f"Attention Heatmap - Layer {layer_idx+1}, Head {head_idx+1}")
    plt.ylabel("Query Tokens")
    plt.xlabel("Key Tokens")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    return plt

def plot_attention_across_layers_heads(attention_matrices, tokens, selected_token_idx=None):
    """Plot average attention for all layers and heads, with optional focus on a specific token."""
    num_layers = len(attention_matrices)
    num_heads = attention_matrices[0].shape[1]
    
    layer_head_attention = np.zeros((num_layers, num_heads))
    
    # Calculate average attention or attention for specific token
    for layer_idx in range(num_layers):
        for head_idx in range(num_heads):
            if selected_token_idx is not None:
                # Focus on attention from the selected token to all other tokens
                layer_head_attention[layer_idx, head_idx] = attention_matrices[layer_idx][0, head_idx, selected_token_idx].mean()
            else:
                # Average attention across all tokens
                layer_head_attention[layer_idx, head_idx] = attention_matrices[layer_idx][0, head_idx].mean()
    
    # Create custom colormap: blue for low values, red for high values
    custom_cmap = LinearSegmentedColormap.from_list('blue_red', ['blue', 'white', 'red'], N=256)
    
    # Plot heatmap
    plt.figure(figsize=(12, 8))
    
    if selected_token_idx is not None:
        title = f"Attention from token '{tokens[selected_token_idx]}' across layers and heads"
    else:
        title = "Average attention across layers and heads"
    
    sns.heatmap(layer_head_attention, cmap=custom_cmap, 
                xticklabels=[f"Head {i+1}" for i in range(num_heads)],
                yticklabels=[f"Layer {i+1}" for i in range(num_layers)])
    
    plt.title(title)
    plt.ylabel("Layers")
    plt.xlabel("Attention Heads")
    plt.tight_layout()
    
    return plt

def analyze_sentiment_with_attention(text):
    """Analyze sentiment and visualize attention patterns for the given text."""
    print(f"Analyzing sentiment for: '{text}'")
    
    # Get sentiment prediction
    sentiment_result = sentiment_pipeline(text)[0]
    print(f"Sentiment: {sentiment_result['label']} (Score: {sentiment_result['score']:.4f})")
    
    # Get attention scores
    attention_matrices, tokens, logits = get_attention_scores(text, model, tokenizer)
    
    # Print tokens for reference
    print("\nTokens:")
    for i, token in enumerate(tokens):
        print(f"{i}: {token}")
    
    # Number of layers and heads
    num_layers = len(attention_matrices)
    num_heads = attention_matrices[0].shape[1]
    print(f"\nModel has {num_layers} layers with {num_heads} attention heads each")
    
    # Plot average attention across all layers and heads
    plt_avg = plot_attention_across_layers_heads(attention_matrices, tokens)
    plt_avg.savefig("avg_attention_heatmap.png")
    plt_avg.show()
    
    # Find important tokens based on average attention
    avg_attention_per_token = np.zeros(len(tokens))
    for layer_idx in range(num_layers):
        for head_idx in range(num_heads):
            avg_attention_per_token += attention_matrices[layer_idx][0, head_idx].mean(axis=0)
    
    # Get indices of top 3 tokens by attention
    top_token_indices = np.argsort(avg_attention_per_token)[-3:][::-1]
    print("\nTop tokens by attention:")
    for idx in top_token_indices:
        print(f"Token '{tokens[idx]}' (index {idx}) - Average attention: {avg_attention_per_token[idx]:.4f}")
    
    # Plot attention from the most important token
    most_important_token_idx = top_token_indices[0]
    plt_token = plot_attention_across_layers_heads(attention_matrices, tokens, most_important_token_idx)
    plt_token.savefig(f"token_{most_important_token_idx}_attention.png")
    plt_token.show()
    
    # Plot attention heatmaps for specific layers and heads
    # Select a few interesting layers/heads to visualize
    interesting_combinations = [
        (0, 0),  # First layer, first head
        (num_layers // 2, num_heads // 2),  # Middle layer, middle head
        (num_layers - 1, num_heads - 1)  # Last layer, last head
    ]
    
    for layer_idx, head_idx in interesting_combinations:
        plt_heatmap = plot_attention_heatmap(attention_matrices, tokens, layer_idx, head_idx)
        plt_heatmap.savefig(f"attention_layer{layer_idx+1}_head{head_idx+1}.png")
        plt_heatmap.show()
    
    # Plot attention across tokens for specific layers
    special_layers = [0, num_layers//2, num_layers-1]  # First, middle, last layers
    
    plt.figure(figsize=(15, 10))
    for i, layer_idx in enumerate(special_layers):
        avg_attention = np.mean(attention_matrices[layer_idx][0], axis=0)  # Average across heads
        plt.subplot(len(special_layers), 1, i+1)
        plt.bar(range(len(tokens)), avg_attention.mean(axis=0))
        plt.title(f"Layer {layer_idx+1} - Average attention per token")
        plt.xticks(range(len(tokens)), tokens, rotation=90)
        plt.tight_layout()
    
    plt.savefig("attention_per_token.png")
    plt.show()
    
    return attention_matrices, tokens

###################################################################
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import XLMRobertaModel, XLMRobertaTokenizer
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
class XLMRobertaLayerwiseClassifier(nn.Module):
    def __init__(self, model_name="xlm-roberta-base", num_classes=3):
        super(XLMRobertaLayerwiseClassifier, self).__init__()
        self.num_classes = num_classes
        
        # Load pre-trained XLM-RoBERTa model
        self.roberta = XLMRobertaModel.from_pretrained(model_name, output_hidden_states=True)
        
        # Get the dimension of hidden states
        self.hidden_size = self.roberta.config.hidden_size
        
        # Create a classification head for each layer
        num_layers = self.roberta.config.num_hidden_layers + 1  # +1 for embedding layer
        self.classifiers = nn.ModuleList([
            nn.Linear(self.hidden_size, num_classes) for _ in range(num_layers)
        ])
        
    def forward(self, input_ids, attention_mask=None):
        # Get all hidden states from the model
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        
        # Get all hidden states (including embeddings)
        all_hidden_states = outputs.hidden_states
        
        # For each layer, apply its corresponding classifier and softmax
        layer_predictions = []
        
        for layer_idx, hidden_state in enumerate(all_hidden_states):
            # Use CLS token representation [batch_size, hidden_size]
            cls_output = hidden_state[:, 0, :]
            
            # Apply linear layer for this specific layer
            logits = self.classifiers[layer_idx](cls_output)
            
            # Apply softmax to get probabilities
            probs = F.softmax(logits, dim=1)
            
            layer_predictions.append(probs)
            
        return layer_predictions

def analyze_layer_predictions(text, model, tokenizer, class_names, device):
    # Prepare input
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    
    # Get predictions for each layer
    with torch.no_grad():
        layer_predictions = model(input_ids=inputs["input_ids"], 
                                  attention_mask=inputs["attention_mask"])
    
    # Convert to numpy for easier handling
    layer_predictions_np = [pred.cpu().numpy()[0] for pred in layer_predictions]
    
    # Create a DataFrame for easier visualization
    layers = [f"Layer {i}" if i > 0 else "Embedding" for i in range(len(layer_predictions_np))]
    
    # Create a dictionary for the DataFrame
    data = {
        "Layer": layers
    }
    
    for i, class_name in enumerate(class_names):
        data[class_name] = [layer_pred[i] for layer_pred in layer_predictions_np]
    
    df = pd.DataFrame(data)
    
    return df

def visualize_layer_predictions(df, class_names):
    # Melt the DataFrame for easier plotting
    df_melted = pd.melt(df, id_vars=["Layer"], value_vars=class_names, 
                        var_name="Class", value_name="Probability")
    
    # Plot
    plt.figure(figsize=(12, 6))
    sns.lineplot(data=df_melted, x="Layer", y="Probability", hue="Class", marker="o")
    plt.title("Class Probabilities Across Layers")
    plt.xticks(rotation=45)
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.tight_layout()
    plt.show()
    
    # Heatmap for another perspective
    plt.figure(figsize=(12, 6))
    pivot_df = df.set_index("Layer")
    sns.heatmap(pivot_df, annot=True, cmap="YlGnBu", fmt=".2f")
    plt.title("Class Probabilities Heatmap Across Layers")
    plt.tight_layout()
    plt.show()
    
    # Show predicted class at each layer
    predicted_classes = []
    for _, row in df.iterrows():
        class_probs = [row[class_name] for class_name in class_names]
        predicted_class = class_names[np.argmax(class_probs)]
        predicted_classes.append(predicted_class)
    
    df["Predicted Class"] = predicted_classes
    
    # Print when prediction changes
    print("Layer-wise Predictions:")
    prev_class = None
    for i, pred_class in enumerate(predicted_classes):
        layer_name = df.iloc[i]["Layer"]
        if pred_class != prev_class:
            print(f"{layer_name}: {pred_class} (Change)")
        else:
            print(f"{layer_name}: {pred_class}")
        prev_class = pred_class
    
    return df


#################
import torch
import numpy as np
from transformers import XLMRobertaTokenizer, XLMRobertaModel
import matplotlib.pyplot as plt

# Load pre-trained XLM-RoBERTa model and tokenizer
tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')
model = XLMRobertaModel.from_pretrained('xlm-roberta-base', output_attentions=True)

def get_word_level_attention(sentences, target_word):
    """
    Extract word-level attention matrix from the final layer for a target word 
    across a group of sentences.
    
    Args:
        sentences (list): List of input sentences
        target_word (str): Target word to extract attention for
    
    Returns:
        dict: Word-level attention scores for each sentence
    """
    word_level_attentions = {}
    
    for i, sentence in enumerate(sentences):
        # Tokenize the sentence
        tokens = tokenizer.tokenize(sentence)
        token_ids = tokenizer.encode(sentence, return_tensors='pt')
        
        # Get the mapping from tokens to original words
        words = sentence.split()
        word_to_tokens = {}
        
        # Get the tokens without special tokens for mapping
        tokens_without_special = tokenizer.tokenize(sentence)
        
        # Map words to their corresponding tokens
        word_idx = 0
        token_idx = 0
        current_word = ""
        
        while word_idx < len(words) and token_idx < len(tokens_without_special):
            if tokens_without_special[token_idx].startswith('▁'):  # New word starts with this token
                current_word = words[word_idx]
                if current_word not in word_to_tokens:
                    word_to_tokens[current_word] = []
                word_to_tokens[current_word].append(token_idx + 1)  # +1 for [CLS] token
                
                # Check if this token represents the complete word or just a part
                if current_word == tokens_without_special[token_idx].replace('▁', ''):
                    word_idx += 1
                    token_idx += 1
                else:
                    token_idx += 1
            else:  # Continuation of previous word
                if current_word not in word_to_tokens:
                    word_to_tokens[current_word] = []
                word_to_tokens[current_word].append(token_idx + 1)  # +1 for [CLS] token
                token_idx += 1
                
                # Check if we've completed the current word
                reconstructed = ''.join([tokens_without_special[t].replace('▁', '') 
                                        for t in [t-1 for t in word_to_tokens[current_word]]])
                if reconstructed == current_word:
                    word_idx += 1
        
        # Run the model and get attention
        with torch.no_grad():
            outputs = model(token_ids)
            
        # Get the last layer's attention matrix
        # Shape: [batch_size=1, num_heads, seq_length, seq_length]
        last_layer_attention = outputs.attentions[-1].squeeze(0)
        
        # Average over attention heads
        # Shape: [seq_length, seq_length]
        avg_attention = last_layer_attention.mean(dim=0)
        
        # If the target word is present in the sentence
        if target_word in word_to_tokens:
            target_token_indices = word_to_tokens[target_word]
            
            # Get the attention from the target word to all other tokens
            # First, average the attention from all subwords of the target word
            target_attention = avg_attention[target_token_indices].mean(dim=0)
            
            # Convert token-level attention to word-level attention
            word_attention = {}
            for word, token_indices in word_to_tokens.items():
                # Average attention to all subwords of this word
                word_attention[word] = target_attention[token_indices].mean().item()
            
            word_level_attentions[i] = {
                'sentence': sentence,
                'attention': word_attention
            }
    
    return word_level_attentions

def visualize_word_attention(word_level_attentions, target_word):
    """
    Visualize word-level attention for a target word across sentences.
    
    Args:
        word_level_attentions (dict): Word-level attention scores
        target_word (str): Target word
    """
    plt.figure(figsize=(12, 8))
    
    for idx, data in word_level_attentions.items():
        sentence = data['sentence']
        attention = data['attention']
        
        # Sort words by attention score
        words = list(attention.keys())
        scores = [attention[w] for w in words]
        
        sorted_indices = np.argsort(scores)[::-1]  # Sort in descending order
        sorted_words = [words[i] for i in sorted_indices]
        sorted_scores = [scores[i] for i in sorted_indices]
        
        plt.subplot(len(word_level_attentions), 1, idx + 1)
        plt.barh(sorted_words, sorted_scores)
        plt.title(f"Sentence {idx+1}: {sentence}")
        plt.xlabel("Attention Score")
        plt.ylabel("Words")
        plt.tight_layout()
    
    plt.suptitle(f"Word-level attention for '{target_word}'")
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()

# Example usage
sentences = [
    "The new artificial intelligence model performs well on multiple tasks.",
    "Researchers developed an advanced AI system with improved capabilities.",
    "The performance of the model exceeded our expectations."
]

target_word = "model"
word_attentions = get_word_level_attention(sentences, target_word)

# Calculate global importance across all sentences
all_words = set()
for data in word_attentions.values():
    all_words.update(data['attention'].keys())

global_importance = {word: 0 for word in all_words}
count = {word: 0 for word in all_words}

for data in word_attentions.values():
    for word, score in data['attention'].items():
        global_importance[word] += score
        count[word] += 1

# Average the scores
for word in global_importance:
    if count[word] > 0:
        global_importance[word] /= count[word]

# Print global importance
print(f"Global importance of features related to '{target_word}':")
sorted_words = sorted(global_importance.items(), key=lambda x: x[1], reverse=True)
for word, score in sorted_words:
    print(f"{word}: {score:.4f}")

# Visualize the attention for each sentence
visualize_word_attention(word_attentions, target_word)

###############
import numpy as np
import matplotlib.pyplot as plt

def simulate_distribution(n, N, num_samples=100000, seed=42):
    np.random.seed(seed)
    
    p = n / N
    q = 1 - p

    # Define support
    k_vals = np.arange(0, N + 1)

    # Compute unnormalized probabilities
    unnormalized_probs = p * (q ** k_vals)

    # Normalize
    normalization = 1 - q ** (N + 1)
    probs = unnormalized_probs / normalization

    # Simulate samples
    samples = np.random.choice(k_vals, size=num_samples, p=probs)

    # Confidence intervals
    ci_95 = np.percentile(samples, 95)
    ci_99 = np.percentile(samples, 99)

    # Plot histogram
    plt.figure(figsize=(10, 6))
    plt.hist(samples, bins=range(N + 2), density=True, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(ci_95, color='red', linestyle='--', label=f'95% right-sided CI = {int(ci_95)}')
    plt.axvline(ci_99, color='purple', linestyle='--', label=f'99% right-sided CI = {int(ci_99)}')
    plt.title(f"Simulated Distribution of K (n={n}, N={N})")
    plt.xlabel("k")
    plt.ylabel("Probability")
    plt.legend()
    plt.grid(True)
    plt.show()

    return {
        "distribution": list(zip(k_vals, probs)),
        "samples": samples,
        "ci_95": int(ci_95),
        "ci_99": int(ci_99)
    }

# Example usage
result = simulate_distribution(n=100, N=10000)
print(f"95% right-sided confidence limit: {result['ci_95']}")
print(f"99% right-sided confidence limit: {result['ci_99']}")
