import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification
from transformers import pipeline
from bertviz import head_view
import pandas as pd
from matplotlib.colors import LinearSegmentedColormap

# Set up the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the model and tokenizer
model_name = "xlm-roberta-base"
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)
model = XLMRobertaForSequenceClassification.from_pretrained(model_name, num_labels=3)
model.to(device)

# Create a sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)

def get_attention_scores(text, model, tokenizer):
    """Get attention scores for all layers and heads from the model."""
    # Tokenize input
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Get model outputs with attention matrices
    with torch.no_grad():
        outputs = model(**inputs, output_attentions=True)
    
    # Get attention outputs and convert to numpy arrays
    attentions = outputs.attentions
    attention_matrices = [attention.cpu().numpy() for attention in attentions]
##############################
def consolidate_vectors(key_to_words, word_to_vector, target_key, aggregation_method='sum'):
    """
    Consolidate vectors for all words associated with a target key.
    
    Parameters:
    - key_to_words: Dictionary mapping keys to lists of words
    - word_to_vector: Dictionary mapping words to vector tuples
    - target_key: The key whose associated word vectors will be consolidated
    - aggregation_method: How to combine vectors ('sum', 'average', 'max', 'min')
    
    Returns:
    - Consolidated vector as a tuple
    """
    # Get all words associated with the target key
    if target_key not in key_to_words:
        return None
    
    words = key_to_words[target_key]
    
    # Collect vectors for all valid words
    vectors = []
    for word in words:
        if word in word_to_vector:
            vectors.append(word_to_vector[word])
    
    if not vectors:
        return None
    
    # Convert tuples to lists for easier manipulation
    vector_lists = [list(v) for v in vectors]
    
    # Get the length of vectors (assuming all have same length)
    vector_length = len(vector_lists[0])
    
    # Perform the aggregation based on the specified method
    if aggregation_method == 'sum':
        result = [sum(v[i] for v in vector_lists) for i in range(vector_length)]
    elif aggregation_method == 'average':
        result = [sum(v[i] for v in vector_lists) / len(vectors) for i in range(vector_length)]
    elif aggregation_method == 'max':
        result = [max(v[i] for v in vector_lists) for i in range(vector_length)]
    elif aggregation_method == 'min':
        result = [min(v[i] for v in vector_lists) for i in range(vector_length)]
    else:
        raise ValueError(f"Unknown aggregation method: {aggregation_method}")
    
    return tuple(result)
Here's an example of how to use this function:
python# Example dictionaries
key_to_words = {
    'animals': ['cat', 'dog', 'bird'],
    'colors': ['red', 'blue', 'green'],
    'numbers': ['one', 'two', 'three']
}

word_to_vector = {
    'cat': (1, 2, 3),
    'dog': (4, 5, 6),
    'bird': (7, 8, 9),
    'red': (10, 20, 30),
    'blue': (40, 50, 60),
    'one': (100, 200, 300)
}

# Consolidate vectors for 'animals' key using default 'sum' method
result = consolidate_vectors(key_to_words, word_to_vector, 'animals')
print(f"Sum of vectors for 'animals': {result}")
    
    # Get token ids and convert to tokens
    input_ids = inputs["input_ids"][0].cpu().numpy()
    tokens = tokenizer.convert_ids_to_tokens(input_ids)
    
    return attention_matrices, tokens, outputs.logits

def plot_attention_heatmap(attention_matrices, tokens, layer_idx, head_idx, cmap='viridis'):
    """Plot attention heatmap for a specific layer and attention head."""
    # Get attention matrix for specific layer and head
    attention = attention_matrices[layer_idx][0, head_idx]
    
    # Create a figure
    plt.figure(figsize=(12, 10))
    
    # Plot heatmap
    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap=cmap)
    
    plt.title(f"Attention Heatmap - Layer {layer_idx+1}, Head {head_idx+1}")
    plt.ylabel("Query Tokens")
    plt.xlabel("Key Tokens")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    return plt

def plot_attention_across_layers_heads(attention_matrices, tokens, selected_token_idx=None):
    """Plot average attention for all layers and heads, with optional focus on a specific token."""
    num_layers = len(attention_matrices)
    num_heads = attention_matrices[0].shape[1]
    
    layer_head_attention = np.zeros((num_layers, num_heads))
    
    # Calculate average attention or attention for specific token
    for layer_idx in range(num_layers):
        for head_idx in range(num_heads):
            if selected_token_idx is not None:
                # Focus on attention from the selected token to all other tokens
                layer_head_attention[layer_idx, head_idx] = attention_matrices[layer_idx][0, head_idx, selected_token_idx].mean()
            else:
                # Average attention across all tokens
                layer_head_attention[layer_idx, head_idx] = attention_matrices[layer_idx][0, head_idx].mean()
    
    # Create custom colormap: blue for low values, red for high values
    custom_cmap = LinearSegmentedColormap.from_list('blue_red', ['blue', 'white', 'red'], N=256)
    
    # Plot heatmap
    plt.figure(figsize=(12, 8))
    
    if selected_token_idx is not None:
        title = f"Attention from token '{tokens[selected_token_idx]}' across layers and heads"
    else:
        title = "Average attention across layers and heads"
    
    sns.heatmap(layer_head_attention, cmap=custom_cmap, 
                xticklabels=[f"Head {i+1}" for i in range(num_heads)],
                yticklabels=[f"Layer {i+1}" for i in range(num_layers)])
    
    plt.title(title)
    plt.ylabel("Layers")
    plt.xlabel("Attention Heads")
    plt.tight_layout()
    
    return plt

def analyze_sentiment_with_attention(text):
    """Analyze sentiment and visualize attention patterns for the given text."""
    print(f"Analyzing sentiment for: '{text}'")
    
    # Get sentiment prediction
    sentiment_result = sentiment_pipeline(text)[0]
    print(f"Sentiment: {sentiment_result['label']} (Score: {sentiment_result['score']:.4f})")
    
    # Get attention scores
    attention_matrices, tokens, logits = get_attention_scores(text, model, tokenizer)
    
    # Print tokens for reference
    print("\nTokens:")
    for i, token in enumerate(tokens):
        print(f"{i}: {token}")
    
    # Number of layers and heads
    num_layers = len(attention_matrices)
    num_heads = attention_matrices[0].shape[1]
    print(f"\nModel has {num_layers} layers with {num_heads} attention heads each")
    
    # Plot average attention across all layers and heads
    plt_avg = plot_attention_across_layers_heads(attention_matrices, tokens)
    plt_avg.savefig("avg_attention_heatmap.png")
    plt_avg.show()
    
    # Find important tokens based on average attention
    avg_attention_per_token = np.zeros(len(tokens))
    for layer_idx in range(num_layers):
        for head_idx in range(num_heads):
            avg_attention_per_token += attention_matrices[layer_idx][0, head_idx].mean(axis=0)
    
    # Get indices of top 3 tokens by attention
    top_token_indices = np.argsort(avg_attention_per_token)[-3:][::-1]
    print("\nTop tokens by attention:")
    for idx in top_token_indices:
        print(f"Token '{tokens[idx]}' (index {idx}) - Average attention: {avg_attention_per_token[idx]:.4f}")
    
    # Plot attention from the most important token
    most_important_token_idx = top_token_indices[0]
    plt_token = plot_attention_across_layers_heads(attention_matrices, tokens, most_important_token_idx)
    plt_token.savefig(f"token_{most_important_token_idx}_attention.png")
    plt_token.show()
    
    # Plot attention heatmaps for specific layers and heads
    # Select a few interesting layers/heads to visualize
    interesting_combinations = [
        (0, 0),  # First layer, first head
        (num_layers // 2, num_heads // 2),  # Middle layer, middle head
        (num_layers - 1, num_heads - 1)  # Last layer, last head
    ]
    
    for layer_idx, head_idx in interesting_combinations:
        plt_heatmap = plot_attention_heatmap(attention_matrices, tokens, layer_idx, head_idx)
        plt_heatmap.savefig(f"attention_layer{layer_idx+1}_head{head_idx+1}.png")
        plt_heatmap.show()
    
    # Plot attention across tokens for specific layers
    special_layers = [0, num_layers//2, num_layers-1]  # First, middle, last layers
    
    plt.figure(figsize=(15, 10))
    for i, layer_idx in enumerate(special_layers):
        avg_attention = np.mean(attention_matrices[layer_idx][0], axis=0)  # Average across heads
        plt.subplot(len(special_layers), 1, i+1)
        plt.bar(range(len(tokens)), avg_attention.mean(axis=0))
        plt.title(f"Layer {layer_idx+1} - Average attention per token")
        plt.xticks(range(len(tokens)), tokens, rotation=90)
        plt.tight_layout()
    
    plt.savefig("attention_per_token.png")
    plt.show()
    
    return attention_matrices, tokens

###################################################################
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import XLMRobertaModel, XLMRobertaTokenizer
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
class XLMRobertaLayerwiseClassifier(nn.Module):
    def __init__(self, model_name="xlm-roberta-base", num_classes=3):
        super(XLMRobertaLayerwiseClassifier, self).__init__()
        self.num_classes = num_classes
        
        # Load pre-trained XLM-RoBERTa model
        self.roberta = XLMRobertaModel.from_pretrained(model_name, output_hidden_states=True)
        
        # Get the dimension of hidden states
        self.hidden_size = self.roberta.config.hidden_size
        
        # Create a classification head for each layer
        num_layers = self.roberta.config.num_hidden_layers + 1  # +1 for embedding layer
        self.classifiers = nn.ModuleList([
            nn.Linear(self.hidden_size, num_classes) for _ in range(num_layers)
        ])
        
    def forward(self, input_ids, attention_mask=None):
        # Get all hidden states from the model
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        
        # Get all hidden states (including embeddings)
        all_hidden_states = outputs.hidden_states
        
        # For each layer, apply its corresponding classifier and softmax
        layer_predictions = []
        
        for layer_idx, hidden_state in enumerate(all_hidden_states):
            # Use CLS token representation [batch_size, hidden_size]
            cls_output = hidden_state[:, 0, :]
            
            # Apply linear layer for this specific layer
            logits = self.classifiers[layer_idx](cls_output)
            
            # Apply softmax to get probabilities
            probs = F.softmax(logits, dim=1)
            
            layer_predictions.append(probs)
            
        return layer_predictions

def analyze_layer_predictions(text, model, tokenizer, class_names, device):
    # Prepare input
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
    
    # Get predictions for each layer
    with torch.no_grad():
        layer_predictions = model(input_ids=inputs["input_ids"], 
                                  attention_mask=inputs["attention_mask"])
    
    # Convert to numpy for easier handling
    layer_predictions_np = [pred.cpu().numpy()[0] for pred in layer_predictions]
    
    # Create a DataFrame for easier visualization
    layers = [f"Layer {i}" if i > 0 else "Embedding" for i in range(len(layer_predictions_np))]
    
    # Create a dictionary for the DataFrame
    data = {
        "Layer": layers
    }
    
    for i, class_name in enumerate(class_names):
        data[class_name] = [layer_pred[i] for layer_pred in layer_predictions_np]
    
    df = pd.DataFrame(data)
    
    return df

def visualize_layer_predictions(df, class_names):
    # Melt the DataFrame for easier plotting
    df_melted = pd.melt(df, id_vars=["Layer"], value_vars=class_names, 
                        var_name="Class", value_name="Probability")
    
    # Plot
    plt.figure(figsize=(12, 6))
    sns.lineplot(data=df_melted, x="Layer", y="Probability", hue="Class", marker="o")
    plt.title("Class Probabilities Across Layers")
    plt.xticks(rotation=45)
    plt.grid(True, linestyle="--", alpha=0.7)
    plt.tight_layout()
    plt.show()
    
    # Heatmap for another perspective
    plt.figure(figsize=(12, 6))
    pivot_df = df.set_index("Layer")
    sns.heatmap(pivot_df, annot=True, cmap="YlGnBu", fmt=".2f")
    plt.title("Class Probabilities Heatmap Across Layers")
    plt.tight_layout()
    plt.show()
    
    # Show predicted class at each layer
    predicted_classes = []
    for _, row in df.iterrows():
        class_probs = [row[class_name] for class_name in class_names]
        predicted_class = class_names[np.argmax(class_probs)]
        predicted_classes.append(predicted_class)
    
    df["Predicted Class"] = predicted_classes
    
    # Print when prediction changes
    print("Layer-wise Predictions:")
    prev_class = None
    for i, pred_class in enumerate(predicted_classes):
        layer_name = df.iloc[i]["Layer"]
        if pred_class != prev_class:
            print(f"{layer_name}: {pred_class} (Change)")
        else:
            print(f"{layer_name}: {pred_class}")
        prev_class = pred_class
    
    return df
